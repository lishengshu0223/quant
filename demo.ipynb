{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from my_model.linear_model import layer_embedding as MyModel\n",
    "from my_dataset.linear_dataset import MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "code        000001.SZ  000002.SZ  000004.SZ  000005.SZ  000006.SZ  000007.SZ  \\\ndt                                                                             \n2014-01-02  -0.030448  -0.069008   0.025657  -0.044194  -0.136403   0.005676   \n2014-01-03  -0.007562  -0.054992  -0.052687  -0.053074  -0.120228   0.002179   \n2014-01-06   0.003476  -0.033638  -0.057617  -0.033409  -0.058411  -0.017157   \n2014-01-07   0.005150  -0.032215  -0.061517  -0.032860  -0.035923  -0.048517   \n2014-01-08  -0.002579  -0.033638  -0.051468  -0.029227  -0.022742  -0.039024   \n\ncode        000008.SZ  000009.SZ  000010.SZ  000011.SZ  ...  688786.SH  \\\ndt                                                      ...              \n2014-01-02  -0.099164  -0.079422  -0.115093  -0.092042  ...        NaN   \n2014-01-03  -0.090949  -0.054661  -0.069328  -0.094981  ...        NaN   \n2014-01-06  -0.012993  -0.033251  -0.032283  -0.063117  ...        NaN   \n2014-01-07   0.035773  -0.027421  -0.038713  -0.054584  ...        NaN   \n2014-01-08   0.044866   0.006798  -0.074572  -0.032958  ...        NaN   \n\ncode        688787.SH  688788.SH  688789.SH  688793.SH  688798.SH  688799.SH  \\\ndt                                                                             \n2014-01-02        NaN        NaN        NaN        NaN        NaN        NaN   \n2014-01-03        NaN        NaN        NaN        NaN        NaN        NaN   \n2014-01-06        NaN        NaN        NaN        NaN        NaN        NaN   \n2014-01-07        NaN        NaN        NaN        NaN        NaN        NaN   \n2014-01-08        NaN        NaN        NaN        NaN        NaN        NaN   \n\ncode        688800.SH  688819.SH  688981.SH  \ndt                                           \n2014-01-02        NaN        NaN        NaN  \n2014-01-03        NaN        NaN        NaN  \n2014-01-06        NaN        NaN        NaN  \n2014-01-07        NaN        NaN        NaN  \n2014-01-08        NaN        NaN        NaN  \n\n[5 rows x 5012 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>code</th>\n      <th>000001.SZ</th>\n      <th>000002.SZ</th>\n      <th>000004.SZ</th>\n      <th>000005.SZ</th>\n      <th>000006.SZ</th>\n      <th>000007.SZ</th>\n      <th>000008.SZ</th>\n      <th>000009.SZ</th>\n      <th>000010.SZ</th>\n      <th>000011.SZ</th>\n      <th>...</th>\n      <th>688786.SH</th>\n      <th>688787.SH</th>\n      <th>688788.SH</th>\n      <th>688789.SH</th>\n      <th>688793.SH</th>\n      <th>688798.SH</th>\n      <th>688799.SH</th>\n      <th>688800.SH</th>\n      <th>688819.SH</th>\n      <th>688981.SH</th>\n    </tr>\n    <tr>\n      <th>dt</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2014-01-02</th>\n      <td>-0.030448</td>\n      <td>-0.069008</td>\n      <td>0.025657</td>\n      <td>-0.044194</td>\n      <td>-0.136403</td>\n      <td>0.005676</td>\n      <td>-0.099164</td>\n      <td>-0.079422</td>\n      <td>-0.115093</td>\n      <td>-0.092042</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2014-01-03</th>\n      <td>-0.007562</td>\n      <td>-0.054992</td>\n      <td>-0.052687</td>\n      <td>-0.053074</td>\n      <td>-0.120228</td>\n      <td>0.002179</td>\n      <td>-0.090949</td>\n      <td>-0.054661</td>\n      <td>-0.069328</td>\n      <td>-0.094981</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2014-01-06</th>\n      <td>0.003476</td>\n      <td>-0.033638</td>\n      <td>-0.057617</td>\n      <td>-0.033409</td>\n      <td>-0.058411</td>\n      <td>-0.017157</td>\n      <td>-0.012993</td>\n      <td>-0.033251</td>\n      <td>-0.032283</td>\n      <td>-0.063117</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2014-01-07</th>\n      <td>0.005150</td>\n      <td>-0.032215</td>\n      <td>-0.061517</td>\n      <td>-0.032860</td>\n      <td>-0.035923</td>\n      <td>-0.048517</td>\n      <td>0.035773</td>\n      <td>-0.027421</td>\n      <td>-0.038713</td>\n      <td>-0.054584</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2014-01-08</th>\n      <td>-0.002579</td>\n      <td>-0.033638</td>\n      <td>-0.051468</td>\n      <td>-0.029227</td>\n      <td>-0.022742</td>\n      <td>-0.039024</td>\n      <td>0.044866</td>\n      <td>0.006798</td>\n      <td>-0.074572</td>\n      <td>-0.032958</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 5012 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factor_concat = pd.read_pickle(\"./data/factor_concat_2018_2019.pkl\")\n",
    "stock_return = pd.read_pickle(\"./data/stock_return.pkl\")\n",
    "stock_return.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "dataset = MyDataset(factor_concat, stock_return)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=False, drop_last=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "layer_embedding(\n  (embedding): Embedding(3028, 10)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=985, out_features=256, bias=True)\n    (1): ReLU()\n    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Linear(in_features=256, out_features=256, bias=True)\n    (4): ReLU()\n    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): Linear(in_features=256, out_features=256, bias=True)\n    (7): ReLU()\n    (8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): Linear(in_features=256, out_features=1, bias=True)\n  )\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "feature_num = len(factor_concat.columns.levels[0])\n",
    "stock_num = len(dataset.num_code_dict)\n",
    "model = MyModel(975,3028,10)\n",
    "model.load_state_dict(torch.load('./log/18_eb/model_30.pth'))\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 670/2824 [00:11<00:35, 60.23it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_24236\\923978535.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[0my_pred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstock_id\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m     \u001B[0my_pred\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msigmoid\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_pred\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"cpu\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdate_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m         \u001B[0mdate\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnum_date_dict\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mdate_list\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "score = stock_return.copy()\n",
    "score.iloc[:,:] = np.nan\n",
    "score_dict = {}\n",
    "for date in dataset.date_num_dict.keys():\n",
    "    score_dict[date] = {}\n",
    "for date_num, code_num, x, y in tqdm(dataloader):\n",
    "    date_list = date_num.numpy().flatten()\n",
    "    code_list = code_num.numpy().flatten()\n",
    "    stock_id = code_num.int().to(device)\n",
    "    x = x.float().to(device)\n",
    "    y_pred = model(stock_id, x)\n",
    "    y_pred = torch.sigmoid(y_pred).to(\"cpu\").detach().numpy().flatten()\n",
    "    for i in range(len(date_list)):\n",
    "        date = dataset.num_date_dict[date_list[i]]\n",
    "        code = dataset.num_code_dict[code_list[i]]\n",
    "        score_dict[date][code] = y_pred[i]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score = pd.DataFrame(score_dict)\n",
    "score.reindex(stock_return.columns)\n",
    "score = score.sort_index().T\n",
    "score.to_pickle(\"F:\\Multifactor_Project\\score_18.pkl\")\n",
    "score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "dt          code     \n2014-01-02  000001.SZ    1\n            000002.SZ    1\n            000004.SZ    0\n            000005.SZ    1\n            000006.SZ    0\n                        ..\n2022-12-22  688798.SH    1\n            688799.SH    1\n            688800.SH    1\n            688819.SH    0\n            688981.SH    1\nLength: 7740431, dtype: category\nCategories (2, int64): [0 < 1]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "stock_price = pd.read_pickle(\"F:\\Trade_data\\\\adjopen.pkl\")\n",
    "stock_return = stock_price.pct_change().shift(-6)\n",
    "quantile = 2\n",
    "return_stack = stock_return.stack().dropna()\n",
    "quantile_return = return_stack.groupby(\"dt\").apply(\n",
    "    lambda x: pd.qcut(\n",
    "        x, np.arange(quantile + 1) / quantile, np.arange(quantile)\n",
    "    )\n",
    ")\n",
    "quantile_return[quantile_return < (quantile - 1)] = 0\n",
    "quantile_return[quantile_return == (quantile - 1)] = 1\n",
    "quantile_return.to_pickle(\"./data/quantile_return.pkl\")\n",
    "quantile_return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "from my_dataset.linear_dataset import make_idx_num_dict\n",
    "date_num_dict, num_date_dict = make_idx_num_dict(stock_price.index)\n",
    "code_num_dict, num_code_dict = make_idx_num_dict(stock_price.columns)\n",
    "import pickle\n",
    "\n",
    "with open('./data/processed_data/date_num_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(date_num_dict, f)\n",
    "with open('./data/processed_data/num_date_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(num_date_dict, f)\n",
    "with open('./data/processed_data/code_num_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(code_num_dict, f)\n",
    "with open('./data/processed_data/num_code_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(num_code_dict, f)\n",
    "print(\"finished\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "1681"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# code_num_dict = pd.read_pickle(\"./data/processed_data/date_num_dict.pkl\")\n",
    "code_num_dict['300136.SZ']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}