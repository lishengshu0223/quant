{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2015: 100%|██████████| 982/982 [04:13<00:00,  3.87it/s]\n",
      "2016: 100%|██████████| 982/982 [04:22<00:00,  3.74it/s]\n",
      "2017: 100%|██████████| 982/982 [04:22<00:00,  3.73it/s]\n",
      "2018: 100%|██████████| 982/982 [04:18<00:00,  3.80it/s]\n",
      "2019: 100%|██████████| 982/982 [04:25<00:00,  3.71it/s]\n",
      "2020: 100%|██████████| 982/982 [04:29<00:00,  3.64it/s]\n",
      "2021: 100%|██████████| 982/982 [04:28<00:00,  3.66it/s]\n",
      "2022: 100%|██████████| 982/982 [04:25<00:00,  3.71it/s]\n",
      "2023: 100%|██████████| 982/982 [04:25<00:00,  3.70it/s]\n"
     ]
    }
   ],
   "source": [
    "path = \"F:\\Pfactor_clean\"\n",
    "file_name_list = os.listdir(path)\n",
    "for year in ['2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']:\n",
    "    factor_concat = {}\n",
    "    pbar = tqdm(file_name_list)\n",
    "    for file_name in pbar:\n",
    "        pbar.set_description(f\"{year}\")\n",
    "        factor_name = file_name[:-8]\n",
    "        factor = pd.read_feather(f\"{path}\\\\{file_name}\").set_index('dt')\n",
    "        factor = factor.loc[f\"{year}-01-01\":f\"{year}-12-31\", :]\n",
    "        factor_concat[factor_name] = factor\n",
    "    factor_concat = pd.concat(factor_concat, axis=1)\n",
    "    factor_concat.to_pickle(f\"./data/factor_concat_{year}.pkl\")\n",
    "del factor_concat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n"
     ]
    }
   ],
   "source": [
    "quantile = 2\n",
    "stock_return = pd.read_pickle(\"./data/stock_return.pkl\")\n",
    "for year in ['2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023']:\n",
    "    print(year)\n",
    "    factor_stack = pd.read_pickle(f\"./data/factor_concat_{year}.pkl\").stack()\n",
    "    quantile_return = stock_return.stack()\n",
    "    quantile_return = quantile_return.groupby('dt').apply(\n",
    "        lambda x: pd.qcut(x, np.arange(quantile + 1) / quantile, np.arange(quantile))\n",
    "    )\n",
    "    common_idx = np.intersect1d(factor_stack.index, quantile_return.index)\n",
    "    factor_stack = factor_stack.loc[common_idx, :]\n",
    "    quantile_return = quantile_return[common_idx]\n",
    "    factor_stack.to_pickle(f\"./data/processed_data/factor_stack_{year}.pkl\")\n",
    "    quantile_return.to_pickle(f\"./data/processed_data/quantile_return_{year}.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "dt          code     \n2023-01-03  000001.SZ    SZ\n            000002.SZ    SZ\n            000006.SZ    SZ\n            000007.SZ    SZ\n            000008.SZ    SZ\n                         ..\n2023-03-29  688798.SH    SH\n            688799.SH    SH\n            688800.SH    SH\n            688819.SH    SH\n            688981.SH    SH\nName: code, Length: 252265, dtype: object"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# factor_stack = pd.read_pickle(f\"./data/processed_data/factor_stack_2023.pkl\")\n",
    "code_num_dict = pd.read_pickle(f\"./data/processed_data/code_num_dict.pkl\")\n",
    "factor_stack['code'] = factor_stack.reset_index()['code'].str.split('.', expand=True)[1].values\n",
    "factor_stack.iloc[:,982]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "            000001.SZ  000002.SZ  000004.SZ  000005.SZ  000006.SZ  000007.SZ  \\\ndt                                                                             \n2023-03-27  -0.001581  -0.001949   0.011051  -0.010791  -0.039486  -0.021656   \n2023-03-28  -0.010211  -0.017520  -0.022167  -0.011453  -0.040195  -0.011728   \n2023-03-29   0.001595  -0.007793  -0.019236  -0.005518  -0.009866  -0.023299   \n2023-03-30  -0.003942   0.009618   0.025745   0.005549   0.019821  -0.011835   \n2023-03-31   0.011952   0.023620  -0.016041  -0.010976  -0.009807  -0.023513   \n\n            000008.SZ  000009.SZ  000010.SZ  000011.SZ  ...  688786.SH  \\\ndt                                                      ...              \n2023-03-27  -0.008148  -0.010336  -0.041426  -0.017294  ...  -0.038314   \n2023-03-28   0.008215   0.000889  -0.045917  -0.017806  ...  -0.038921   \n2023-03-29   0.008510   0.010705  -0.077223  -0.008732  ...  -0.056249   \n2023-03-30   0.025553   0.003452  -0.039399   0.019888  ...  -0.042899   \n2023-03-31  -0.004331   0.000888  -0.089168   0.012930  ...  -0.022401   \n\n            688787.SH  688788.SH  688789.SH  688793.SH  688798.SH  688799.SH  \\\ndt                                                                             \n2023-03-27   0.114078  -0.004125  -0.067471  -0.053966  -0.006448   0.014540   \n2023-03-28   0.007145   0.012863  -0.022989  -0.041505   0.117735  -0.003170   \n2023-03-29  -0.168016   0.142500  -0.035468   0.008901   0.137796  -0.001567   \n2023-03-30  -0.009663   0.130799  -0.045632  -0.014048   0.138248   0.032687   \n2023-03-31  -0.154377   0.086582  -0.045515  -0.047043   0.123679   0.049176   \n\n            688800.SH  688819.SH  688981.SH  \ndt                                           \n2023-03-27  -0.023818   0.012867   0.090452  \n2023-03-28  -0.036197  -0.003668   0.168937  \n2023-03-29  -0.003536  -0.013972   0.220522  \n2023-03-30   0.041307  -0.023289   0.228106  \n2023-03-31   0.053028  -0.021342   0.192132  \n\n[5 rows x 5081 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>000001.SZ</th>\n      <th>000002.SZ</th>\n      <th>000004.SZ</th>\n      <th>000005.SZ</th>\n      <th>000006.SZ</th>\n      <th>000007.SZ</th>\n      <th>000008.SZ</th>\n      <th>000009.SZ</th>\n      <th>000010.SZ</th>\n      <th>000011.SZ</th>\n      <th>...</th>\n      <th>688786.SH</th>\n      <th>688787.SH</th>\n      <th>688788.SH</th>\n      <th>688789.SH</th>\n      <th>688793.SH</th>\n      <th>688798.SH</th>\n      <th>688799.SH</th>\n      <th>688800.SH</th>\n      <th>688819.SH</th>\n      <th>688981.SH</th>\n    </tr>\n    <tr>\n      <th>dt</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2023-03-27</th>\n      <td>-0.001581</td>\n      <td>-0.001949</td>\n      <td>0.011051</td>\n      <td>-0.010791</td>\n      <td>-0.039486</td>\n      <td>-0.021656</td>\n      <td>-0.008148</td>\n      <td>-0.010336</td>\n      <td>-0.041426</td>\n      <td>-0.017294</td>\n      <td>...</td>\n      <td>-0.038314</td>\n      <td>0.114078</td>\n      <td>-0.004125</td>\n      <td>-0.067471</td>\n      <td>-0.053966</td>\n      <td>-0.006448</td>\n      <td>0.014540</td>\n      <td>-0.023818</td>\n      <td>0.012867</td>\n      <td>0.090452</td>\n    </tr>\n    <tr>\n      <th>2023-03-28</th>\n      <td>-0.010211</td>\n      <td>-0.017520</td>\n      <td>-0.022167</td>\n      <td>-0.011453</td>\n      <td>-0.040195</td>\n      <td>-0.011728</td>\n      <td>0.008215</td>\n      <td>0.000889</td>\n      <td>-0.045917</td>\n      <td>-0.017806</td>\n      <td>...</td>\n      <td>-0.038921</td>\n      <td>0.007145</td>\n      <td>0.012863</td>\n      <td>-0.022989</td>\n      <td>-0.041505</td>\n      <td>0.117735</td>\n      <td>-0.003170</td>\n      <td>-0.036197</td>\n      <td>-0.003668</td>\n      <td>0.168937</td>\n    </tr>\n    <tr>\n      <th>2023-03-29</th>\n      <td>0.001595</td>\n      <td>-0.007793</td>\n      <td>-0.019236</td>\n      <td>-0.005518</td>\n      <td>-0.009866</td>\n      <td>-0.023299</td>\n      <td>0.008510</td>\n      <td>0.010705</td>\n      <td>-0.077223</td>\n      <td>-0.008732</td>\n      <td>...</td>\n      <td>-0.056249</td>\n      <td>-0.168016</td>\n      <td>0.142500</td>\n      <td>-0.035468</td>\n      <td>0.008901</td>\n      <td>0.137796</td>\n      <td>-0.001567</td>\n      <td>-0.003536</td>\n      <td>-0.013972</td>\n      <td>0.220522</td>\n    </tr>\n    <tr>\n      <th>2023-03-30</th>\n      <td>-0.003942</td>\n      <td>0.009618</td>\n      <td>0.025745</td>\n      <td>0.005549</td>\n      <td>0.019821</td>\n      <td>-0.011835</td>\n      <td>0.025553</td>\n      <td>0.003452</td>\n      <td>-0.039399</td>\n      <td>0.019888</td>\n      <td>...</td>\n      <td>-0.042899</td>\n      <td>-0.009663</td>\n      <td>0.130799</td>\n      <td>-0.045632</td>\n      <td>-0.014048</td>\n      <td>0.138248</td>\n      <td>0.032687</td>\n      <td>0.041307</td>\n      <td>-0.023289</td>\n      <td>0.228106</td>\n    </tr>\n    <tr>\n      <th>2023-03-31</th>\n      <td>0.011952</td>\n      <td>0.023620</td>\n      <td>-0.016041</td>\n      <td>-0.010976</td>\n      <td>-0.009807</td>\n      <td>-0.023513</td>\n      <td>-0.004331</td>\n      <td>0.000888</td>\n      <td>-0.089168</td>\n      <td>0.012930</td>\n      <td>...</td>\n      <td>-0.022401</td>\n      <td>-0.154377</td>\n      <td>0.086582</td>\n      <td>-0.045515</td>\n      <td>-0.047043</td>\n      <td>0.123679</td>\n      <td>0.049176</td>\n      <td>0.053028</td>\n      <td>-0.021342</td>\n      <td>0.192132</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 5081 columns</p>\n</div>"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_return = pd.read_pickle(\"./data/stock_return.pkl\")\n",
    "stock_return.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "dt          code     \n2014-01-02  000001.SZ    1\n            000002.SZ    0\n            000004.SZ    1\n            000005.SZ    1\n            000006.SZ    0\ndtype: category\nCategories (2, int64): [0 < 1]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile = 2\n",
    "quantile_return = stock_return.stack()\n",
    "quantile_return = quantile_return.groupby('dt').apply(\n",
    "    lambda x: pd.qcut(x, np.arange(quantile + 1) / quantile, np.arange(quantile))\n",
    ")\n",
    "quantile_return.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "common_idx = np.intersect1d(factor_stack.index, quantile_return.index)\n",
    "factor_stack = factor_stack.loc[common_idx, :]\n",
    "quantile_return = quantile_return[common_idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69339 | val_0_auc: 0.5389  |  0:00:34s\n",
      "epoch 1  | loss: 0.68992 | val_0_auc: 0.56007 |  0:01:08s\n",
      "epoch 2  | loss: 0.68702 | val_0_auc: 0.57157 |  0:01:42s\n",
      "epoch 3  | loss: 0.68439 | val_0_auc: 0.58092 |  0:02:16s\n",
      "epoch 4  | loss: 0.68192 | val_0_auc: 0.58843 |  0:02:50s\n",
      "epoch 5  | loss: 0.67983 | val_0_auc: 0.59319 |  0:03:24s\n",
      "epoch 6  | loss: 0.67751 | val_0_auc: 0.5999  |  0:03:58s\n",
      "epoch 7  | loss: 0.67582 | val_0_auc: 0.60352 |  0:04:32s\n",
      "epoch 8  | loss: 0.67577 | val_0_auc: 0.58879 |  0:05:06s\n",
      "epoch 9  | loss: 0.67399 | val_0_auc: 0.6069  |  0:05:40s\n",
      "epoch 10 | loss: 0.67214 | val_0_auc: 0.61027 |  0:06:15s\n",
      "epoch 11 | loss: 0.67071 | val_0_auc: 0.60868 |  0:06:49s\n",
      "epoch 12 | loss: 0.67006 | val_0_auc: 0.61155 |  0:07:23s\n",
      "epoch 13 | loss: 0.66903 | val_0_auc: 0.61227 |  0:07:57s\n",
      "epoch 14 | loss: 0.66902 | val_0_auc: 0.59271 |  0:08:31s\n",
      "epoch 15 | loss: 0.67005 | val_0_auc: 0.61259 |  0:09:05s\n",
      "epoch 16 | loss: 0.66721 | val_0_auc: 0.61322 |  0:09:39s\n",
      "epoch 17 | loss: 0.66643 | val_0_auc: 0.61387 |  0:10:13s\n",
      "epoch 18 | loss: 0.66605 | val_0_auc: 0.61552 |  0:10:49s\n",
      "epoch 19 | loss: 0.66535 | val_0_auc: 0.61425 |  0:11:23s\n",
      "epoch 20 | loss: 0.67152 | val_0_auc: 0.59113 |  0:11:58s\n",
      "epoch 21 | loss: 0.66806 | val_0_auc: 0.61643 |  0:12:32s\n",
      "epoch 22 | loss: 0.66449 | val_0_auc: 0.61637 |  0:13:06s\n",
      "epoch 23 | loss: 0.66404 | val_0_auc: 0.61794 |  0:13:41s\n",
      "epoch 24 | loss: 0.66359 | val_0_auc: 0.61867 |  0:14:15s\n",
      "epoch 25 | loss: 0.66337 | val_0_auc: 0.61729 |  0:14:50s\n",
      "epoch 26 | loss: 0.66297 | val_0_auc: 0.61844 |  0:15:24s\n",
      "epoch 27 | loss: 0.66264 | val_0_auc: 0.61794 |  0:15:58s\n",
      "epoch 28 | loss: 0.66213 | val_0_auc: 0.62067 |  0:16:32s\n",
      "epoch 29 | loss: 0.66192 | val_0_auc: 0.62024 |  0:17:05s\n",
      "Stop training because you reached max_epochs = 30 with best_epoch = 28 and best_val_0_auc = 0.62067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at ./log/tabnet/2018.zip\n",
      "2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69287 | val_0_auc: 0.5391  |  0:00:38s\n",
      "epoch 1  | loss: 0.69056 | val_0_auc: 0.54573 |  0:01:16s\n",
      "epoch 2  | loss: 0.68966 | val_0_auc: 0.55876 |  0:01:55s\n",
      "epoch 3  | loss: 0.68752 | val_0_auc: 0.57156 |  0:02:34s\n",
      "epoch 4  | loss: 0.68476 | val_0_auc: 0.57812 |  0:03:13s\n",
      "epoch 5  | loss: 0.6824  | val_0_auc: 0.58715 |  0:03:52s\n",
      "epoch 6  | loss: 0.68039 | val_0_auc: 0.59245 |  0:04:31s\n",
      "epoch 7  | loss: 0.67932 | val_0_auc: 0.5946  |  0:05:10s\n",
      "epoch 8  | loss: 0.67774 | val_0_auc: 0.59772 |  0:05:48s\n",
      "epoch 9  | loss: 0.67708 | val_0_auc: 0.60005 |  0:06:28s\n",
      "epoch 10 | loss: 0.67547 | val_0_auc: 0.60073 |  0:07:06s\n",
      "epoch 11 | loss: 0.67451 | val_0_auc: 0.60227 |  0:07:45s\n",
      "epoch 12 | loss: 0.67367 | val_0_auc: 0.60328 |  0:08:22s\n",
      "epoch 13 | loss: 0.67305 | val_0_auc: 0.60515 |  0:09:00s\n",
      "epoch 14 | loss: 0.67225 | val_0_auc: 0.60514 |  0:09:38s\n",
      "epoch 15 | loss: 0.67162 | val_0_auc: 0.60461 |  0:10:16s\n",
      "epoch 16 | loss: 0.67102 | val_0_auc: 0.6073  |  0:10:53s\n",
      "epoch 17 | loss: 0.67059 | val_0_auc: 0.60745 |  0:11:31s\n",
      "epoch 18 | loss: 0.67009 | val_0_auc: 0.60738 |  0:12:09s\n",
      "epoch 19 | loss: 0.66966 | val_0_auc: 0.60914 |  0:12:47s\n",
      "epoch 20 | loss: 0.66924 | val_0_auc: 0.60776 |  0:13:24s\n",
      "epoch 21 | loss: 0.669   | val_0_auc: 0.60995 |  0:14:02s\n",
      "epoch 22 | loss: 0.66847 | val_0_auc: 0.6105  |  0:14:40s\n",
      "epoch 23 | loss: 0.66811 | val_0_auc: 0.61024 |  0:15:18s\n",
      "epoch 24 | loss: 0.66799 | val_0_auc: 0.60966 |  0:15:56s\n",
      "epoch 25 | loss: 0.66752 | val_0_auc: 0.61122 |  0:16:33s\n",
      "epoch 26 | loss: 0.6672  | val_0_auc: 0.61164 |  0:17:11s\n",
      "epoch 27 | loss: 0.66701 | val_0_auc: 0.61162 |  0:17:48s\n",
      "epoch 28 | loss: 0.66663 | val_0_auc: 0.61129 |  0:18:26s\n",
      "epoch 29 | loss: 0.6663  | val_0_auc: 0.61259 |  0:19:04s\n",
      "Stop training because you reached max_epochs = 30 with best_epoch = 29 and best_val_0_auc = 0.61259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at ./log/tabnet/2019.zip\n",
      "2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69289 | val_0_auc: 0.53463 |  0:01:36s\n",
      "epoch 1  | loss: 0.69118 | val_0_auc: 0.53456 |  0:03:11s\n",
      "epoch 2  | loss: 0.69109 | val_0_auc: 0.53943 |  0:04:47s\n",
      "epoch 3  | loss: 0.68993 | val_0_auc: 0.54863 |  0:06:22s\n",
      "epoch 4  | loss: 0.68895 | val_0_auc: 0.55781 |  0:07:57s\n",
      "epoch 5  | loss: 0.68738 | val_0_auc: 0.56573 |  0:09:31s\n",
      "epoch 6  | loss: 0.68631 | val_0_auc: 0.57121 |  0:11:07s\n",
      "epoch 7  | loss: 0.68567 | val_0_auc: 0.57076 |  0:12:44s\n",
      "epoch 8  | loss: 0.68427 | val_0_auc: 0.57764 |  0:14:20s\n",
      "epoch 9  | loss: 0.68304 | val_0_auc: 0.57803 |  0:15:57s\n",
      "epoch 10 | loss: 0.68254 | val_0_auc: 0.58056 |  0:17:31s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_8952\\3420604620.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     16\u001B[0m         \u001B[0mdevice_name\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'cuda'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m     )\n\u001B[1;32m---> 18\u001B[1;33m     model.fit(x_train, y_train,\n\u001B[0m\u001B[0;32m     19\u001B[0m               \u001B[0meval_set\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_valid\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_valid\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m               \u001B[0mdrop_last\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, X_train, y_train, eval_set, eval_name, eval_metric, loss_fn, weights, max_epochs, patience, batch_size, virtual_batch_size, num_workers, drop_last, callbacks, pin_memory, from_unsupervised, warm_start, augmentations)\u001B[0m\n\u001B[0;32m    239\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_callback_container\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_epoch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepoch_idx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    240\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 241\u001B[1;33m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_train_epoch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_dataloader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    242\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    243\u001B[0m             \u001B[1;31m# Apply predict epoch to all eval sets\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py\u001B[0m in \u001B[0;36m_train_epoch\u001B[1;34m(self, train_loader)\u001B[0m\n\u001B[0;32m    455\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_callback_container\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch_idx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    456\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 457\u001B[1;33m             \u001B[0mbatch_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_train_batch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    458\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    459\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_callback_container\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_batch_end\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch_idx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_logs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py\u001B[0m in \u001B[0;36m_train_batch\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    502\u001B[0m         \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    503\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclip_value\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 504\u001B[1;33m             \u001B[0mclip_grad_norm_\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnetwork\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclip_value\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    505\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_optimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    506\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py\u001B[0m in \u001B[0;36mclip_grad_norm_\u001B[1;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001B[0m\n\u001B[0;32m     41\u001B[0m         \u001B[0mtotal_norm\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnorms\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnorms\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m1\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnorms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 43\u001B[1;33m         \u001B[0mtotal_norm\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnorm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnorm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnorm_type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mg\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mgrads\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnorm_type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     44\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0merror_if_nonfinite\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlogical_or\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtotal_norm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misnan\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtotal_norm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misinf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     45\u001B[0m         raise RuntimeError(\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     41\u001B[0m         \u001B[0mtotal_norm\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnorms\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnorms\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m1\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnorms\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 43\u001B[1;33m         \u001B[0mtotal_norm\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnorm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnorm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnorm_type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mg\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mgrads\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnorm_type\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     44\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0merror_if_nonfinite\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlogical_or\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtotal_norm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misnan\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtotal_norm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misinf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     45\u001B[0m         raise RuntimeError(\n",
      "\u001B[1;32mE:\\Anaconda\\lib\\site-packages\\torch\\functional.py\u001B[0m in \u001B[0;36mnorm\u001B[1;34m(input, p, dim, keepdim, out, dtype)\u001B[0m\n\u001B[0;32m   1483\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1484\u001B[0m             \u001B[0m_dim\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mndim\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m  \u001B[1;31m# noqa: C416 TODO: rewrite as list(range(m))\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1485\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0m_VF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnorm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0m_dim\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkeepdim\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mkeepdim\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore[attr-defined]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1486\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1487\u001B[0m     \u001B[1;31m# TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "max_epochs = 30\n",
    "for year in [2018, 2019, 2020, 2021, 2022]:\n",
    "    print(year)\n",
    "    x_train = pd.read_pickle(f\"./data/processed_data/factor_stack_{year - 3}.pkl\")\n",
    "    x_train = pd.concat([x_train, pd.read_pickle(f\"./data/processed_data/factor_stack_{year - 2}.pkl\")])\n",
    "    x_train = pd.concat([x_train, pd.read_pickle(f\"./data/processed_data/factor_stack_{year - 1}.pkl\")])\n",
    "    y_train = pd.read_pickle(f\"./data/processed_data/quantile_return_{year - 3}.pkl\")\n",
    "    y_train = pd.concat([y_train, pd.read_pickle(f\"./data/processed_data/quantile_return_{year - 2}.pkl\")])\n",
    "    y_train = pd.concat([y_train, pd.read_pickle(f\"./data/processed_data/quantile_return_{year - 1}.pkl\")])\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_train.values, y_train.values)\n",
    "    model = TabNetClassifier(\n",
    "        device_name='cuda'\n",
    "    )\n",
    "    model.fit(x_train, y_train,\n",
    "              eval_set=[(x_valid, y_valid)],\n",
    "              drop_last=False,\n",
    "              max_epochs=max_epochs)\n",
    "    model.save_model(f\"./log/tabnet/{year}\")\n",
    "    del x_train, x_valid, y_train, y_valid\n",
    "\n",
    "    x_test = pd.read_pickle(f\"./data/processed_data/factor_stack_{year}.pkl\")\n",
    "    score = model.predict_proba(x_test.values)\n",
    "    score = pd.DataFrame(score[:, 1], index=pd.MultiIndex.from_tuples(x_test.index))\n",
    "    score.index.names = ['dt', 'code']\n",
    "    score = score.unstack()\n",
    "    score.columns = score.columns.droplevel(0)\n",
    "    score.to_pickle(f\"F:\\Multifactor_Project\\score_{year}.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "982"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "model = TabNetClassifier(device_name='cuda')\n",
    "model.load_model(\"./log/tabnet/basic/2018.zip\")\n",
    "model.input_dim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "data": {
      "text/plain": "dt                   \n2023-01-03  000001.SZ    34\n            000002.SZ    15\n            000006.SZ    15\n            000007.SZ     8\n            000008.SZ    17\n                         ..\n2023-03-29  688798.SH    22\n            688799.SH     7\n            688800.SH    22\n            688819.SH    21\n            688981.SH    22\nLength: 252265, dtype: int64"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "industry_info = pd.read_pickle(\"F:\\Trade_data\\\\industry_info.pkl\").stack()\n",
    "industry_list = np.unique(industry_info.values).tolist()\n",
    "industry_info = industry_info.replace(industry_list, [i for i in range(len(industry_list))])\n",
    "industry_info = industry_info.loc[factor_stack.index]\n",
    "industry_info"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1003/1003 [00:04<00:00, 207.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "      alphaP0001  alphaP0002  alphaP0003  alphaP0004  alphaP0005  alphaP0006  \\\nyear                                                                           \n2018    0.063553    0.039901    0.018074    0.073046    0.052584    0.035313   \n2019    0.050471    0.019004    0.011829    0.071386    0.056034    0.036852   \n2020    0.047403    0.014453    0.012370    0.065773    0.057549    0.036298   \n2021    0.048106    0.014698    0.018729    0.065230    0.062265    0.029967   \n2022    0.051454    0.021312    0.026741    0.062490    0.051608    0.025221   \n\n      alphaP0007  alphaP0008  alphaP0009  alphaP0010  ...  alphaP2019_5_20  \\\nyear                                                  ...                    \n2018    0.007961    0.030818    0.006084    0.003972  ...         0.026976   \n2019    0.018776    0.021547    0.008739    0.001909  ...         0.024031   \n2020    0.012923    0.016575    0.011628    0.003454  ...         0.025873   \n2021    0.009449    0.012261    0.009115    0.002975  ...         0.026250   \n2022    0.007967    0.012818    0.006941    0.009698  ...         0.035524   \n\n      alphaP2019_5_200  alphaP2019_5_5  alphaP2019_5_60  alphaP2019_60_10  \\\nyear                                                                        \n2018          0.001015        0.019026         0.007896          0.007249   \n2019          0.009792        0.021794         0.011941          0.010966   \n2020          0.013163        0.023125         0.012916          0.015647   \n2021          0.017083        0.029189         0.019987          0.021343   \n2022          0.021602        0.032481         0.025114          0.024454   \n\n      alphaP2019_60_120  alphaP2019_60_20  alphaP2019_60_200  alphaP2019_60_5  \\\nyear                                                                            \n2018           0.001726          0.020485           0.001474         0.037764   \n2019           0.010908          0.020467           0.009506         0.022140   \n2020           0.014756          0.022562           0.012882         0.016565   \n2021           0.020124          0.023685           0.016778         0.009549   \n2022           0.023682          0.031716           0.021226         0.011082   \n\n      alphaP2019_60_60  \nyear                    \n2018          0.006654  \n2019          0.010892  \n2020          0.011727  \n2021          0.018797  \n2022          0.023775  \n\n[5 rows x 1003 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alphaP0001</th>\n      <th>alphaP0002</th>\n      <th>alphaP0003</th>\n      <th>alphaP0004</th>\n      <th>alphaP0005</th>\n      <th>alphaP0006</th>\n      <th>alphaP0007</th>\n      <th>alphaP0008</th>\n      <th>alphaP0009</th>\n      <th>alphaP0010</th>\n      <th>...</th>\n      <th>alphaP2019_5_20</th>\n      <th>alphaP2019_5_200</th>\n      <th>alphaP2019_5_5</th>\n      <th>alphaP2019_5_60</th>\n      <th>alphaP2019_60_10</th>\n      <th>alphaP2019_60_120</th>\n      <th>alphaP2019_60_20</th>\n      <th>alphaP2019_60_200</th>\n      <th>alphaP2019_60_5</th>\n      <th>alphaP2019_60_60</th>\n    </tr>\n    <tr>\n      <th>year</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018</th>\n      <td>0.063553</td>\n      <td>0.039901</td>\n      <td>0.018074</td>\n      <td>0.073046</td>\n      <td>0.052584</td>\n      <td>0.035313</td>\n      <td>0.007961</td>\n      <td>0.030818</td>\n      <td>0.006084</td>\n      <td>0.003972</td>\n      <td>...</td>\n      <td>0.026976</td>\n      <td>0.001015</td>\n      <td>0.019026</td>\n      <td>0.007896</td>\n      <td>0.007249</td>\n      <td>0.001726</td>\n      <td>0.020485</td>\n      <td>0.001474</td>\n      <td>0.037764</td>\n      <td>0.006654</td>\n    </tr>\n    <tr>\n      <th>2019</th>\n      <td>0.050471</td>\n      <td>0.019004</td>\n      <td>0.011829</td>\n      <td>0.071386</td>\n      <td>0.056034</td>\n      <td>0.036852</td>\n      <td>0.018776</td>\n      <td>0.021547</td>\n      <td>0.008739</td>\n      <td>0.001909</td>\n      <td>...</td>\n      <td>0.024031</td>\n      <td>0.009792</td>\n      <td>0.021794</td>\n      <td>0.011941</td>\n      <td>0.010966</td>\n      <td>0.010908</td>\n      <td>0.020467</td>\n      <td>0.009506</td>\n      <td>0.022140</td>\n      <td>0.010892</td>\n    </tr>\n    <tr>\n      <th>2020</th>\n      <td>0.047403</td>\n      <td>0.014453</td>\n      <td>0.012370</td>\n      <td>0.065773</td>\n      <td>0.057549</td>\n      <td>0.036298</td>\n      <td>0.012923</td>\n      <td>0.016575</td>\n      <td>0.011628</td>\n      <td>0.003454</td>\n      <td>...</td>\n      <td>0.025873</td>\n      <td>0.013163</td>\n      <td>0.023125</td>\n      <td>0.012916</td>\n      <td>0.015647</td>\n      <td>0.014756</td>\n      <td>0.022562</td>\n      <td>0.012882</td>\n      <td>0.016565</td>\n      <td>0.011727</td>\n    </tr>\n    <tr>\n      <th>2021</th>\n      <td>0.048106</td>\n      <td>0.014698</td>\n      <td>0.018729</td>\n      <td>0.065230</td>\n      <td>0.062265</td>\n      <td>0.029967</td>\n      <td>0.009449</td>\n      <td>0.012261</td>\n      <td>0.009115</td>\n      <td>0.002975</td>\n      <td>...</td>\n      <td>0.026250</td>\n      <td>0.017083</td>\n      <td>0.029189</td>\n      <td>0.019987</td>\n      <td>0.021343</td>\n      <td>0.020124</td>\n      <td>0.023685</td>\n      <td>0.016778</td>\n      <td>0.009549</td>\n      <td>0.018797</td>\n    </tr>\n    <tr>\n      <th>2022</th>\n      <td>0.051454</td>\n      <td>0.021312</td>\n      <td>0.026741</td>\n      <td>0.062490</td>\n      <td>0.051608</td>\n      <td>0.025221</td>\n      <td>0.007967</td>\n      <td>0.012818</td>\n      <td>0.006941</td>\n      <td>0.009698</td>\n      <td>...</td>\n      <td>0.035524</td>\n      <td>0.021602</td>\n      <td>0.032481</td>\n      <td>0.025114</td>\n      <td>0.024454</td>\n      <td>0.023682</td>\n      <td>0.031716</td>\n      <td>0.021226</td>\n      <td>0.011082</td>\n      <td>0.023775</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1003 columns</p>\n</div>"
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"F:\\Pfactor_evaluation_results_R5_latest\"\n",
    "file_name_list = os.listdir(path)\n",
    "ic_mean = []\n",
    "for file_name in tqdm(file_name_list):\n",
    "    factor_name = file_name[:-4]\n",
    "    evaluation = pd.read_pickle(os.path.join(path,file_name))\n",
    "    ic_series = evaluation['IC_report']['IC_series'].to_frame(factor_name)\n",
    "    ic_series['year'] = ic_series.index.year\n",
    "    ic_series = ic_series.groupby('year').mean().rolling(3).mean().loc[2017:]\n",
    "    ic_series.index = ic_series.index + 1\n",
    "    ic_series = abs(ic_series)\n",
    "    ic_mean.append(ic_series)\n",
    "\n",
    "ic_mean = pd.concat(ic_mean, axis=1)\n",
    "ic_mean"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "selected_factor_dict = {}\n",
    "for year in ic_mean.index:\n",
    "    selected_factor = ic_mean.loc[year]\n",
    "    selected_factor = selected_factor[selected_factor>0.01].index.tolist()\n",
    "    selected_factor = np.intersect1d(factor_stack.columns, selected_factor)\n",
    "    selected_factor_dict[year] = selected_factor\n",
    "\n",
    "pd.to_pickle(selected_factor_dict, \"./data/selected_factor_dict.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['alphaP0001', 'alphaP0002', 'alphaP0003', 'alphaP0004',\n       'alphaP0005', 'alphaP0006', 'alphaP0008', 'alphaP0012',\n       'alphaP0013', 'alphaP1001_10', 'alphaP1001_120', 'alphaP1001_20',\n       'alphaP1001_200', 'alphaP1001_5', 'alphaP1001_60', 'alphaP1002_10',\n       'alphaP1002_120', 'alphaP1002_20', 'alphaP1002_200',\n       'alphaP1002_5', 'alphaP1002_60', 'alphaP1003_120', 'alphaP1004_10',\n       'alphaP1004_120', 'alphaP1004_20', 'alphaP1004_200',\n       'alphaP1004_5', 'alphaP1004_60', 'alphaP1005_10', 'alphaP1005_120',\n       'alphaP1005_20', 'alphaP1005_200', 'alphaP1005_60',\n       'alphaP1006_10', 'alphaP1006_120', 'alphaP1006_20',\n       'alphaP1006_200', 'alphaP1006_5', 'alphaP1006_60', 'alphaP1007_10',\n       'alphaP1007_20', 'alphaP1008_10', 'alphaP1008_120',\n       'alphaP1008_20', 'alphaP1008_200', 'alphaP1008_5', 'alphaP1008_60',\n       'alphaP1009_10', 'alphaP1009_120', 'alphaP1009_20',\n       'alphaP1009_200', 'alphaP1009_5', 'alphaP1009_60', 'alphaP1010_10',\n       'alphaP1010_120', 'alphaP1010_20', 'alphaP1010_200',\n       'alphaP1010_5', 'alphaP1010_60', 'alphaP1011_10', 'alphaP1011_120',\n       'alphaP1011_20', 'alphaP1011_200', 'alphaP1011_5', 'alphaP1011_60',\n       'alphaP1012_10', 'alphaP1012_20', 'alphaP1012_200', 'alphaP1012_5',\n       'alphaP1013_10', 'alphaP1013_20', 'alphaP1013_200', 'alphaP1013_5',\n       'alphaP1014_10', 'alphaP1014_20', 'alphaP1014_200', 'alphaP1014_5',\n       'alphaP1016_10', 'alphaP1016_120', 'alphaP1016_20',\n       'alphaP1016_200', 'alphaP1016_5', 'alphaP1016_60', 'alphaP1017_10',\n       'alphaP1017_20', 'alphaP1017_200', 'alphaP1017_5', 'alphaP1018_10',\n       'alphaP1018_120', 'alphaP1018_20', 'alphaP1018_200',\n       'alphaP1018_5', 'alphaP1018_60', 'alphaP1019_120', 'alphaP1019_20',\n       'alphaP1019_200', 'alphaP1019_60', 'alphaP1021_10',\n       'alphaP1021_120', 'alphaP1021_20', 'alphaP1021_200',\n       'alphaP1021_5', 'alphaP1021_60', 'alphaP1022_10', 'alphaP1022_120',\n       'alphaP1022_20', 'alphaP1022_200', 'alphaP1022_5', 'alphaP1022_60',\n       'alphaP1023_10', 'alphaP1023_120', 'alphaP1023_20',\n       'alphaP1023_200', 'alphaP1023_5', 'alphaP1023_60', 'alphaP1025_10',\n       'alphaP1025_120', 'alphaP1025_20', 'alphaP1025_200',\n       'alphaP1025_5', 'alphaP1025_60', 'alphaP1026_10', 'alphaP1026_120',\n       'alphaP1026_20', 'alphaP1026_200', 'alphaP1026_5', 'alphaP1027_10',\n       'alphaP1027_120', 'alphaP1027_20', 'alphaP1027_200',\n       'alphaP1027_5', 'alphaP1027_60', 'alphaP1028_10', 'alphaP1028_120',\n       'alphaP1028_20', 'alphaP1028_5', 'alphaP1028_60', 'alphaP1029_10',\n       'alphaP1029_120', 'alphaP1029_20', 'alphaP1029_200',\n       'alphaP1029_5', 'alphaP1029_60', 'alphaP1030_10', 'alphaP1030_120',\n       'alphaP1030_20', 'alphaP1030_200', 'alphaP1030_5', 'alphaP1030_60',\n       'alphaP1031_10', 'alphaP1031_120', 'alphaP1031_20',\n       'alphaP1031_200', 'alphaP1031_60', 'alphaP1032_10',\n       'alphaP1032_120', 'alphaP1032_20', 'alphaP1032_200',\n       'alphaP1032_5', 'alphaP1032_60', 'alphaP1033_10', 'alphaP1033_120',\n       'alphaP1033_20', 'alphaP1033_200', 'alphaP1033_60',\n       'alphaP1034_10', 'alphaP1034_120', 'alphaP1034_20',\n       'alphaP1034_200', 'alphaP1034_5', 'alphaP1034_60', 'alphaP1035_10',\n       'alphaP1035_120', 'alphaP1035_20', 'alphaP1035_200',\n       'alphaP1035_5', 'alphaP1035_60', 'alphaP1036_10', 'alphaP1036_120',\n       'alphaP1036_20', 'alphaP1036_200', 'alphaP1036_5', 'alphaP1036_60',\n       'alphaP1037_10', 'alphaP1037_120', 'alphaP1037_20',\n       'alphaP1037_200', 'alphaP1037_5', 'alphaP1037_60', 'alphaP1038_10',\n       'alphaP1038_120', 'alphaP1038_20', 'alphaP1038_200',\n       'alphaP1038_5', 'alphaP1038_60', 'alphaP1039_10', 'alphaP1039_120',\n       'alphaP1039_20', 'alphaP1039_200', 'alphaP1039_5', 'alphaP1039_60',\n       'alphaP1040_10', 'alphaP1040_120', 'alphaP1040_20',\n       'alphaP1040_200', 'alphaP1040_5', 'alphaP1040_60', 'alphaP1041_10',\n       'alphaP1041_120', 'alphaP1041_20', 'alphaP1041_200',\n       'alphaP1041_5', 'alphaP1041_60', 'alphaP1042_120',\n       'alphaP1042_200', 'alphaP1042_60', 'alphaP1043_10',\n       'alphaP1043_120', 'alphaP1043_20', 'alphaP1043_200',\n       'alphaP1043_5', 'alphaP1043_60', 'alphaP1044_10', 'alphaP1044_5',\n       'alphaP1045_10', 'alphaP1045_120', 'alphaP1045_20',\n       'alphaP1045_200', 'alphaP1045_5', 'alphaP1045_60', 'alphaP1046_10',\n       'alphaP1046_120', 'alphaP1046_20', 'alphaP1046_200',\n       'alphaP1046_60', 'alphaP1047_120', 'alphaP1047_200',\n       'alphaP1048_10', 'alphaP1048_120', 'alphaP1048_20',\n       'alphaP1048_200', 'alphaP1048_60', 'alphaP1049_10',\n       'alphaP1049_120', 'alphaP1049_20', 'alphaP1049_200',\n       'alphaP1049_5', 'alphaP1049_60', 'alphaP1050_10', 'alphaP1050_120',\n       'alphaP1050_20', 'alphaP1050_200', 'alphaP1050_5', 'alphaP1050_60',\n       'alphaP1051_10', 'alphaP1051_120', 'alphaP1051_20',\n       'alphaP1051_200', 'alphaP1051_5', 'alphaP1051_60', 'alphaP1052_10',\n       'alphaP1052_120', 'alphaP1052_20', 'alphaP1052_200',\n       'alphaP1052_5', 'alphaP1052_60', 'alphaP1053_10', 'alphaP1053_120',\n       'alphaP1053_200', 'alphaP1053_5', 'alphaP1053_60', 'alphaP1054_10',\n       'alphaP1054_120', 'alphaP1054_20', 'alphaP1054_200',\n       'alphaP1054_5', 'alphaP1054_60', 'alphaP1055_10', 'alphaP1055_120',\n       'alphaP1055_20', 'alphaP1055_200', 'alphaP1055_5', 'alphaP1055_60',\n       'alphaP1056_10', 'alphaP1056_20', 'alphaP1056_5', 'alphaP1056_60',\n       'alphaP1057_10', 'alphaP1057_120', 'alphaP1057_20',\n       'alphaP1057_200', 'alphaP1057_5', 'alphaP1057_60', 'alphaP1058_10',\n       'alphaP1058_120', 'alphaP1058_20', 'alphaP1058_200',\n       'alphaP1058_5', 'alphaP1058_60', 'alphaP1060_10', 'alphaP1060_120',\n       'alphaP1060_20', 'alphaP1060_200', 'alphaP1060_5', 'alphaP1060_60',\n       'alphaP1061_10', 'alphaP1061_120', 'alphaP1061_20',\n       'alphaP1061_200', 'alphaP1061_5', 'alphaP1061_60', 'alphaP1062_10',\n       'alphaP1062_120', 'alphaP1062_20', 'alphaP1062_200',\n       'alphaP1062_5', 'alphaP1062_60', 'alphaP1063_10', 'alphaP1063_120',\n       'alphaP1063_20', 'alphaP1063_200', 'alphaP1063_60',\n       'alphaP1064_10', 'alphaP1064_120', 'alphaP1064_20',\n       'alphaP1064_200', 'alphaP1064_5', 'alphaP1064_60', 'alphaP1065_10',\n       'alphaP1065_20', 'alphaP1065_5', 'alphaP1065_60', 'alphaP1066_10',\n       'alphaP1066_20', 'alphaP1066_5', 'alphaP1066_60', 'alphaP1067_10',\n       'alphaP1067_120', 'alphaP1067_20', 'alphaP1067_200',\n       'alphaP1067_60', 'alphaP1068_10', 'alphaP1068_20', 'alphaP1068_60',\n       'alphaP1069_10', 'alphaP1069_120', 'alphaP1069_20', 'alphaP1069_5',\n       'alphaP1069_60', 'alphaP1070_10', 'alphaP1070_120',\n       'alphaP1070_20', 'alphaP1070_200', 'alphaP1070_5', 'alphaP1070_60',\n       'alphaP1071_10', 'alphaP1071_120', 'alphaP1071_20', 'alphaP1071_5',\n       'alphaP1071_60', 'alphaP1072_10', 'alphaP1072_120',\n       'alphaP1072_20', 'alphaP1072_200', 'alphaP1072_5', 'alphaP1072_60',\n       'alphaP1073_10', 'alphaP1073_120', 'alphaP1073_20',\n       'alphaP1073_200', 'alphaP1073_5', 'alphaP1073_60', 'alphaP1074_10',\n       'alphaP1074_120', 'alphaP1074_20', 'alphaP1074_200',\n       'alphaP1074_5', 'alphaP1074_60', 'alphaP1075_10', 'alphaP1075_120',\n       'alphaP1075_20', 'alphaP1075_200', 'alphaP1075_5', 'alphaP1075_60',\n       'alphaP1076_10', 'alphaP1076_120', 'alphaP1076_20',\n       'alphaP1076_200', 'alphaP1076_5', 'alphaP1076_60', 'alphaP1077_10',\n       'alphaP1077_120', 'alphaP1077_20', 'alphaP1077_200',\n       'alphaP1077_5', 'alphaP1077_60', 'alphaP1078_10', 'alphaP1078_120',\n       'alphaP1078_20', 'alphaP1078_200', 'alphaP1078_60',\n       'alphaP1079_120', 'alphaP1079_200', 'alphaP1079_5',\n       'alphaP1080_10', 'alphaP1080_120', 'alphaP1080_20',\n       'alphaP1080_200', 'alphaP1080_5', 'alphaP1080_60', 'alphaP1081_10',\n       'alphaP1081_120', 'alphaP1081_20', 'alphaP1081_200',\n       'alphaP1081_5', 'alphaP1081_60', 'alphaP1082_120',\n       'alphaP1082_200', 'alphaP1083_10', 'alphaP1083_120',\n       'alphaP1083_20', 'alphaP1083_200', 'alphaP1083_5', 'alphaP1083_60',\n       'alphaP1084_120', 'alphaP1084_20', 'alphaP1084_200',\n       'alphaP1084_60', 'alphaP1085_10', 'alphaP1085_120',\n       'alphaP1085_20', 'alphaP1085_200', 'alphaP1085_5', 'alphaP1085_60',\n       'alphaP2001_10_10', 'alphaP2001_10_120', 'alphaP2001_10_20',\n       'alphaP2001_10_200', 'alphaP2001_10_5', 'alphaP2001_10_60',\n       'alphaP2001_120_10', 'alphaP2001_120_20', 'alphaP2001_120_5',\n       'alphaP2001_120_60', 'alphaP2001_200_10', 'alphaP2001_200_20',\n       'alphaP2001_200_5', 'alphaP2001_20_10', 'alphaP2001_20_120',\n       'alphaP2001_20_20', 'alphaP2001_20_200', 'alphaP2001_20_5',\n       'alphaP2001_20_60', 'alphaP2001_5_10', 'alphaP2001_5_120',\n       'alphaP2001_5_20', 'alphaP2001_5_200', 'alphaP2001_5_5',\n       'alphaP2001_5_60', 'alphaP2001_60_10', 'alphaP2001_60_120',\n       'alphaP2001_60_20', 'alphaP2001_60_5', 'alphaP2001_60_60',\n       'alphaP2002_120_10', 'alphaP2002_120_20', 'alphaP2002_120_5',\n       'alphaP2002_200_10', 'alphaP2002_200_120', 'alphaP2002_200_20',\n       'alphaP2002_200_5', 'alphaP2002_20_10', 'alphaP2002_60_10',\n       'alphaP2002_60_20', 'alphaP2002_60_5', 'alphaP2003_10_10',\n       'alphaP2003_10_120', 'alphaP2003_10_20', 'alphaP2003_10_5',\n       'alphaP2003_10_60', 'alphaP2003_120_10', 'alphaP2003_120_20',\n       'alphaP2003_120_5', 'alphaP2003_120_60', 'alphaP2003_200_10',\n       'alphaP2003_200_20', 'alphaP2003_200_5', 'alphaP2003_20_10',\n       'alphaP2003_20_120', 'alphaP2003_20_20', 'alphaP2003_20_5',\n       'alphaP2003_20_60', 'alphaP2003_5_10', 'alphaP2003_5_20',\n       'alphaP2003_5_5', 'alphaP2003_5_60', 'alphaP2003_60_10',\n       'alphaP2003_60_20', 'alphaP2003_60_5', 'alphaP2003_60_60',\n       'alphaP2004_10_10', 'alphaP2004_10_120', 'alphaP2004_10_20',\n       'alphaP2004_10_200', 'alphaP2004_10_5', 'alphaP2004_10_60',\n       'alphaP2004_120_10', 'alphaP2004_120_20', 'alphaP2004_120_5',\n       'alphaP2004_200_10', 'alphaP2004_200_5', 'alphaP2004_20_10',\n       'alphaP2004_20_120', 'alphaP2004_20_20', 'alphaP2004_20_5',\n       'alphaP2004_20_60', 'alphaP2004_5_10', 'alphaP2004_5_120',\n       'alphaP2004_5_20', 'alphaP2004_5_200', 'alphaP2004_5_5',\n       'alphaP2004_5_60', 'alphaP2004_60_10', 'alphaP2004_60_20',\n       'alphaP2004_60_5', 'alphaP2005_10_5', 'alphaP2005_120_10',\n       'alphaP2005_120_20', 'alphaP2005_120_5', 'alphaP2005_120_60',\n       'alphaP2005_200_10', 'alphaP2005_200_20', 'alphaP2005_200_5',\n       'alphaP2005_200_60', 'alphaP2005_20_10', 'alphaP2005_20_5',\n       'alphaP2005_60_10', 'alphaP2005_60_20', 'alphaP2005_60_5',\n       'alphaP2006_10_10', 'alphaP2006_10_120', 'alphaP2006_10_20',\n       'alphaP2006_10_200', 'alphaP2006_10_5', 'alphaP2006_10_60',\n       'alphaP2006_120_10', 'alphaP2006_120_20', 'alphaP2006_120_5',\n       'alphaP2006_120_60', 'alphaP2006_200_10', 'alphaP2006_200_20',\n       'alphaP2006_200_5', 'alphaP2006_20_10', 'alphaP2006_20_120',\n       'alphaP2006_20_20', 'alphaP2006_20_200', 'alphaP2006_20_5',\n       'alphaP2006_20_60', 'alphaP2006_5_10', 'alphaP2006_5_120',\n       'alphaP2006_5_20', 'alphaP2006_5_200', 'alphaP2006_5_5',\n       'alphaP2006_5_60', 'alphaP2006_60_10', 'alphaP2006_60_120',\n       'alphaP2006_60_20', 'alphaP2006_60_5', 'alphaP2006_60_60',\n       'alphaP2007_10_5', 'alphaP2007_120_10', 'alphaP2007_120_20',\n       'alphaP2007_120_5', 'alphaP2007_120_60', 'alphaP2007_200_10',\n       'alphaP2007_200_20', 'alphaP2007_200_5', 'alphaP2007_20_10',\n       'alphaP2007_20_5', 'alphaP2007_60_10', 'alphaP2007_60_20',\n       'alphaP2007_60_5', 'alphaP2008_10_120', 'alphaP2008_10_20',\n       'alphaP2008_10_200', 'alphaP2008_10_60', 'alphaP2008_20_120',\n       'alphaP2008_20_200', 'alphaP2008_20_60', 'alphaP2008_5_10',\n       'alphaP2008_5_120', 'alphaP2008_5_20', 'alphaP2008_5_200',\n       'alphaP2008_5_60', 'alphaP2008_60_120', 'alphaP2009_10_5',\n       'alphaP2009_120_10', 'alphaP2009_120_20', 'alphaP2009_120_5',\n       'alphaP2009_120_60', 'alphaP2009_200_10', 'alphaP2009_200_120',\n       'alphaP2009_200_20', 'alphaP2009_200_5', 'alphaP2009_200_60',\n       'alphaP2009_20_10', 'alphaP2009_20_5', 'alphaP2009_60_10',\n       'alphaP2009_60_20', 'alphaP2009_60_5', 'alphaP2010_10_5',\n       'alphaP2010_120_10', 'alphaP2010_120_20', 'alphaP2010_120_5',\n       'alphaP2010_120_60', 'alphaP2010_200_10', 'alphaP2010_200_120',\n       'alphaP2010_200_20', 'alphaP2010_200_5', 'alphaP2010_200_60',\n       'alphaP2010_20_10', 'alphaP2010_20_5', 'alphaP2010_60_10',\n       'alphaP2010_60_20', 'alphaP2010_60_5', 'alphaP2011_10_5',\n       'alphaP2011_120_10', 'alphaP2011_120_20', 'alphaP2011_120_5',\n       'alphaP2011_120_60', 'alphaP2011_200_10', 'alphaP2011_200_120',\n       'alphaP2011_200_20', 'alphaP2011_200_5', 'alphaP2011_200_60',\n       'alphaP2011_20_10', 'alphaP2011_20_5', 'alphaP2011_60_10',\n       'alphaP2011_60_20', 'alphaP2011_60_5', 'alphaP2012_10_5',\n       'alphaP2012_120_10', 'alphaP2012_120_20', 'alphaP2012_120_5',\n       'alphaP2012_120_60', 'alphaP2012_200_10', 'alphaP2012_200_120',\n       'alphaP2012_200_20', 'alphaP2012_200_5', 'alphaP2012_200_60',\n       'alphaP2012_20_10', 'alphaP2012_20_5', 'alphaP2012_60_10',\n       'alphaP2012_60_20', 'alphaP2012_60_5', 'alphaP2013_120_10',\n       'alphaP2013_120_20', 'alphaP2013_120_5', 'alphaP2013_120_60',\n       'alphaP2013_200_10', 'alphaP2013_200_120', 'alphaP2013_200_20',\n       'alphaP2013_200_5', 'alphaP2013_200_60', 'alphaP2013_20_10',\n       'alphaP2013_20_5', 'alphaP2013_60_10', 'alphaP2013_60_20',\n       'alphaP2013_60_5', 'alphaP2015_10_10', 'alphaP2015_10_5',\n       'alphaP2015_20_10', 'alphaP2015_20_5', 'alphaP2016_10_10',\n       'alphaP2016_10_120', 'alphaP2016_10_20', 'alphaP2016_10_200',\n       'alphaP2016_10_5', 'alphaP2016_10_60', 'alphaP2016_120_10',\n       'alphaP2016_120_120', 'alphaP2016_120_20', 'alphaP2016_120_200',\n       'alphaP2016_120_5', 'alphaP2016_120_60', 'alphaP2016_200_10',\n       'alphaP2016_200_120', 'alphaP2016_200_20', 'alphaP2016_200_200',\n       'alphaP2016_200_5', 'alphaP2016_200_60', 'alphaP2016_20_10',\n       'alphaP2016_20_120', 'alphaP2016_20_20', 'alphaP2016_20_200',\n       'alphaP2016_20_5', 'alphaP2016_20_60', 'alphaP2016_5_10',\n       'alphaP2016_5_120', 'alphaP2016_5_20', 'alphaP2016_5_200',\n       'alphaP2016_5_60', 'alphaP2016_60_10', 'alphaP2016_60_120',\n       'alphaP2016_60_20', 'alphaP2016_60_200', 'alphaP2016_60_5',\n       'alphaP2016_60_60', 'alphaP2017_10_10', 'alphaP2017_10_120',\n       'alphaP2017_10_20', 'alphaP2017_10_200', 'alphaP2017_10_5',\n       'alphaP2017_10_60', 'alphaP2017_120_10', 'alphaP2017_120_120',\n       'alphaP2017_120_20', 'alphaP2017_120_200', 'alphaP2017_120_5',\n       'alphaP2017_120_60', 'alphaP2017_200_10', 'alphaP2017_200_120',\n       'alphaP2017_200_20', 'alphaP2017_200_200', 'alphaP2017_200_5',\n       'alphaP2017_200_60', 'alphaP2017_20_10', 'alphaP2017_20_120',\n       'alphaP2017_20_20', 'alphaP2017_20_200', 'alphaP2017_20_5',\n       'alphaP2017_20_60', 'alphaP2017_5_10', 'alphaP2017_5_120',\n       'alphaP2017_5_20', 'alphaP2017_5_200', 'alphaP2017_5_5',\n       'alphaP2017_5_60', 'alphaP2017_60_10', 'alphaP2017_60_120',\n       'alphaP2017_60_20', 'alphaP2017_60_200', 'alphaP2017_60_5',\n       'alphaP2017_60_60', 'alphaP2018_10_120', 'alphaP2018_10_200',\n       'alphaP2018_10_60', 'alphaP2018_120_10', 'alphaP2018_120_120',\n       'alphaP2018_120_20', 'alphaP2018_120_200', 'alphaP2018_120_60',\n       'alphaP2018_200_10', 'alphaP2018_200_120', 'alphaP2018_200_20',\n       'alphaP2018_200_200', 'alphaP2018_200_5', 'alphaP2018_200_60',\n       'alphaP2018_20_120', 'alphaP2018_20_20', 'alphaP2018_20_200',\n       'alphaP2018_20_60', 'alphaP2018_5_120', 'alphaP2018_5_200',\n       'alphaP2018_60_10', 'alphaP2018_60_120', 'alphaP2018_60_20',\n       'alphaP2018_60_200', 'alphaP2018_60_60', 'alphaP2019_10_10',\n       'alphaP2019_10_120', 'alphaP2019_10_20', 'alphaP2019_10_200',\n       'alphaP2019_10_5', 'alphaP2019_10_60', 'alphaP2019_120_10',\n       'alphaP2019_120_120', 'alphaP2019_120_20', 'alphaP2019_120_200',\n       'alphaP2019_120_5', 'alphaP2019_120_60', 'alphaP2019_200_10',\n       'alphaP2019_200_120', 'alphaP2019_200_20', 'alphaP2019_200_200',\n       'alphaP2019_200_5', 'alphaP2019_200_60', 'alphaP2019_20_10',\n       'alphaP2019_20_120', 'alphaP2019_20_20', 'alphaP2019_20_200',\n       'alphaP2019_20_5', 'alphaP2019_20_60', 'alphaP2019_5_10',\n       'alphaP2019_5_120', 'alphaP2019_5_20', 'alphaP2019_5_200',\n       'alphaP2019_5_5', 'alphaP2019_5_60', 'alphaP2019_60_10',\n       'alphaP2019_60_120', 'alphaP2019_60_20', 'alphaP2019_60_200',\n       'alphaP2019_60_5', 'alphaP2019_60_60'], dtype=object)"
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_factor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}